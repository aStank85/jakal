# Jakal V0.3.x - Insight Generation and Metrics Expansion

## Release Summary

- Release line: `0.3.x`
- Initial insight release (`0.3.1`): February 13, 2026
- Metrics expansion update (`0.3.2`): February 13, 2026
- Reliability and clutch-interpretability update (`0.3.3`): February 13, 2026
- Polish and consistency update (`0.3.4`): February 13, 2026
- Status: stable

## What Shipped

### Insight Engine

- Added `src/analyzer.py` with deterministic, rule-based insights.
- Standardized insight payload for downstream use:
  - `severity`
  - `category`
  - `message`
  - `evidence`
  - `action`

### Rule Coverage

- Small sample-size caution
- High K/D and low win conversion signal
- Entry efficiency vs aggression mismatch
- Clutch attempt/conversion quality checks
- Teamplay/assist contribution flags
- Teamkill discipline checks
- Efficiency (`wins_per_hour`) and impact-vs-win signals
- Baseline fallback insight when no major flags trigger

## `0.3.2` Expansion Highlights

- Added a broad metrics expansion pack to `src/calculator.py`, including:
  - Combat involvement and weighted contribution metrics
  - Opening duel depth metrics
  - Clutch depth and high-pressure metrics
  - Survival/risk metrics
  - Time-normalized productivity metrics
  - Discipline and confidence weighting metrics
- Expanded insights in `src/analyzer.py` to use new metrics:
  - Opening control
  - High-pressure clutch performance
  - Risk profile
  - Clean-play scoring
  - Confidence-aware messaging
- Updated details flow to recalculate metrics from snapshot data so newest formulas are always available.

## `0.3.3` Reliability and Trust Highlights

- Added centralized thresholds module (`src/thresholds.py`) for analyzer/calculator tuning.
- Added time-played sanity gate:
  - `rounds_per_hour` is computed and flagged unreliable below threshold.
  - Per-hour metrics (including `wins_per_hour`) are suppressed when unreliable.
  - CLI now prints `Rounds/Hour` and an explicit warning when time scope is unreliable.
- Added defensive clutch JSON handling:
  - Missing clutch keys default to `0`.
  - Soft invariant checks for `total` and `lost_total` consistency.
  - Mismatch warnings logged; computed sums are used.
- Expanded clutch metric visibility in player details:
  - `clutch_1v2_success` to `clutch_1v5_success`
  - `high_pressure_attempts`, `high_pressure_wins`
  - `disadv_attempt_share`, `extreme_attempts`
- Added new clutch insights:
  - `clutch_burden` (high disadvantaged clutch load)
  - `extreme_clutch` (frequent 1v4/1v5 states)
- Updated clutch-related evidence lines to include counts and denominators for quick validation.
- Deduplicated discipline output so teamkill and clean-play signals do not double-fire.

### App Integration

- Insights now generate on snapshot ingest.
- Top insight is displayed after successful import.
- Player details view now includes an `INSIGHTS` section.

### Quality and Testing

- Added `tests/test_analyzer.py` for schema and rule behavior checks.
- Added calculator/analyzer coverage for expansion metrics and confidence insights.
- Full test suite passing at release time.

## Next Target (v0.4)

- Export/reporting features
- Saved insight reports per player/snapshot
- Scrape-ready profile routing enhancements

## `0.3.4` Polish and Consistency Highlights

- Added high-pressure minimum attempts gate for insights (`HIGH_PRESSURE_MIN_ATTEMPTS = 10`) in addition to attempt-rate gating.
- Replaced clean-play normalization magic number with `CLEAN_PLAY_NORMALIZATION_RATE` (bound to `TEAMKILL_RATE_MEDIUM`).
- Per-hour metrics now suppress to `None` under unreliable time scope; UI renders these as `N/A`.
- Added `DATA QUALITY` block in player details:
  - Time scope reliability (`OK`/`UNRELIABLE`) with rounds/hour context
  - Clutch totals mismatch visibility
- Added calculator data-quality flags:
  - `clutch_totals_mismatch`
  - `clutch_lost_totals_mismatch`
  - `clutch_totals_unreliable`
- Compare flow now always recalculates metrics from latest snapshots, matching details-view formulas and avoiding stale stored-computed drift.
- Added integration regression coverage proving compare uses recalculated metrics, not stale DB computed rows.


# JAKAL v0.3.5.0 - Stack Analysis

## Overview
v0.3.5.0 adds 5-stack team analysis and 5v5 matchup comparison to JAKAL.

## New Features

### Stack Management
Three stack modes:
- Named Stack - Persistent, user-created team groups
- Quick Stack - Temporary analysis, deleted on exit
- Tagged Stack - Auto-built from player tags (placeholder for future tagging system)

### Team Analysis
Analyzes a stack of players and generates:
- Role distribution and composition score (0-100)
- Team averages (K/D, Win%, HS%, KPR, APR)
- Entry efficiency and dedicated entry count
- Clutch hierarchy with gap analysis
- Carry player identification and dependency score
- Team strengths, weaknesses, and actionable insights

### 5v5 Matchup Analysis
Compares two stacks head-to-head:
- Category comparison (K/D, Entry, Clutch, Support, HS%, Win Rate)
- Role-by-role matchups
- Outcome prediction with confidence percentage
- Strategic recommendations
- Key battleground identification

## New Files
- src/stack_manager.py - Stack CRUD operations
- src/team_analyzer.py - Team analysis engine
- src/matchup_analyzer.py - Matchup comparison engine
- tests/test_stack_manager.py - Stack manager tests
- tests/test_team_analyzer.py - Team analyzer tests
- tests/test_matchup_analyzer.py - Matchup analyzer tests
- tests/test_integration.py - End-to-end integration tests
- tests/helpers.py - Test data helpers

## Database Additions
- stacks - Named groups of players
- stack_members - Player-stack membership
- stack_analyses - Team-level computed stats
- matchup_analyses - 5v5 comparison results
